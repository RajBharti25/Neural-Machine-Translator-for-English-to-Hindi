{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Machine_translator_with_attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoWk/JyRsAGZIjY9O57jZ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajBharti25/Neural-Machine-Translator-for-English-to-Hindi/blob/master/Neural_Machine_translator_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnd5zfnXGsgm",
        "colab_type": "text"
      },
      "source": [
        "#Neural Machine Translator with attention\n",
        " ##                                              by *RAJ BHARTI*\n",
        "-------------------\n",
        "**Seq2Seq** is a method of encoder-decoder based machine translation that maps an input of sequence to an output of sequence with a tag and attention value. The idea is to use 2 RNN(or LSTM) based network that will work together with a special token and trying to predict the next state sequence from the previous sequence.\n",
        "\n",
        "Seq2Seq Model is a kind of model that use Encoder and a Decoder on top of the model. The Encoder will encode the sentence word by words into an indexed of vocabulary or known words with index, and the decoder will predict the output of the coded input by decoding the input in sequence and will try to use the last input as the next input if its possible. With this method, it is also possible to predict the next input to create a sentence. Each sentence will be assigned a token to mark the end of the sequence. At the end of prediction, there will also be a token to mark the end of the output. So, from the encoder, it will pass a state to the decoder to predict the output.\n",
        "\n",
        "Here is a view of a basic seq2seq model for a bilingual machine translation.\n",
        "<img src=\"https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png\">\n",
        "\n",
        "\n",
        "In this Notebook we are going to design and trained a English to Hindi seq2seq translator model based on encoder decoder with attention mechanism\n",
        "\n",
        "Example of a seq2seq model with attention is below\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNcVHA8-ZmJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16b612e3-3650-460b-f8f4-f97289f7d887"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sttRnD8fcHYw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0f2df9f-be24-4a4a-99d6-433722e32b00"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Neural Machine Translator/nml with attention"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Neural Machine Translator/nml with attention\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8giM44FJhLv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoeclKpeehny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "822c57fb-dde3-4829-c75e-aad2209bedd4"
      },
      "source": [
        "sentence_list=[]\n",
        "file_dir='/content/drive/My Drive/Colab Notebooks/Neural Machine Translator/nml with attention/hin-eng/hin.txt'\n",
        "with open(file_dir) as f:\n",
        "  lines=f.readlines()\n",
        "  for line in lines:\n",
        "    sentence_list.append(line.split('\\t')[0:2])\n",
        "df=pd.DataFrame(sentence_list,columns=['English','Hindi'])\n",
        "\n",
        "#printing\n",
        "print(df.head())\n",
        "print()\n",
        "print(df.tail())\n",
        "print(df.iloc[2777,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  English   Hindi\n",
            "0    Wow!    वाह!\n",
            "1   Help!   बचाओ!\n",
            "2   Jump.   उछलो.\n",
            "3   Jump.   कूदो.\n",
            "4   Jump.  छलांग.\n",
            "\n",
            "                                                English                                              Hindi\n",
            "2773  If you go to that supermarket, you can buy mos...  उस सूपरमार्केट में तुम लगभग कोई भी रोजाने में ...\n",
            "2774  The passengers who were injured in the acciden...  जिन यात्रियों को दुर्घटना मे चोट आई थी उन्हे अ...\n",
            "2775  Democracy is the worst form of government, exc...  लोकतंत्र सरकार का सबसे घिनौना रूप है, अगर बाकी...\n",
            "2776  If my boy had not been killed in the traffic a...  अगर मेरा बेटा ट्रेफ़िक हादसे में नहीं मारा गया...\n",
            "2777  When I was a kid, touching bugs didn't bother ...  जब मैं बच्चा था, मुझे कीड़ों को छूने से कोई पर...\n",
            "जब मैं बच्चा था, मुझे कीड़ों को छूने से कोई परेशानी नहीं होती थी, पर अब मैं उनकी तस्वीरें देखना भी बर्दाश्त नहीं कर सकता।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZA9aUJbC0Zj",
        "colab_type": "text"
      },
      "source": [
        "We have a dataframe of the training data with columns being 'English' and 'Hindi'.Before Tokenizing the texts we need to proprocess it and remove all the punctuarions and extra spaces. It help the model being more accurate and reduces the training time.\n",
        "\n",
        "We will be using the **Regular Expression** i.e Regex library to preprocess the text data to clean it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWFB4eRPG0h9",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "If you look at the text, You will notice some of the almost all the sentences are ended with full stop,question marks, exclamation mark symbols placed just after the prior word. We need to place a space betwen the word and the symbol to further process the sentences.\n",
        "We also need to replace the extra special characters and remove the extra white space between the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwXE5V3zEvf1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "23a1eedd-ad06-4791-cf25-f0e89fbb6fc5"
      },
      "source": [
        "#creating white space between the word and the puctuation characters\n",
        "df['English']=df['English'].apply(lambda x:re.sub(r\"([.!?¿])\",r\" \\1\",x ))\n",
        "df['Hindi']=df['Hindi'].apply(lambda x:re.sub(r\"([.!?¿।])\",r\" \\1\",x ))\n",
        "print(df.iloc[2777,0])\n",
        "print(df.iloc[2777,1])\n",
        "\n",
        "# removing all the extra characters from the sentences with a white space from the English text\n",
        "df['English']=df['English'].apply(lambda x:re.sub(r\"([^a-zA-Z.!?¿]+)\",' ' ,x ))\n",
        "print(df.iloc[2777,0])\n",
        "\n",
        "# Removing Digits, English words and special characters from Hindi Sentences\n",
        "df['Hindi']=df['Hindi'].apply(lambda x:re.sub(r'([A-Za-z0-9२३०८१५७९४६,]+)',' ' ,x ))\n",
        "\n",
        "# we also will have to remove the extra white space\n",
        "df['English']=df['English'].apply(lambda x:re.sub(r'([\" \"]+)',' ',x ))\n",
        "df['Hindi']=df['Hindi'].apply(lambda x:re.sub(r'([\" \"]+)',' ' ,x ))\n",
        "\n",
        "# we will convert english text into lowercase\n",
        "df['English']=df['English'].apply(lambda x:x.lower())\n",
        "\n",
        "#use the strip command to remove the leading and trailinf white space from the sentences\n",
        "df['English']=df['English'].apply(lambda x:x.strip())\n",
        "df['Hindi']=df['Hindi'].apply(lambda x:x.strip())\n",
        "print('\\n',df.iloc[2777,0])\n",
        "print('\\n',df.iloc[2777,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When I was a kid, touching bugs didn't bother me a bit . Now I can hardly stand looking at pictures of them .\n",
            "जब मैं बच्चा था, मुझे कीड़ों को छूने से कोई परेशानी नहीं होती थी, पर अब मैं उनकी तस्वीरें देखना भी बर्दाश्त नहीं कर सकता ।\n",
            "When I was a kid touching bugs didn t bother me a bit . Now I can hardly stand looking at pictures of them .\n",
            "\n",
            " when i was a kid touching bugs didn t bother me a bit . now i can hardly stand looking at pictures of them .\n",
            "\n",
            " जब मैं बच्चा था मुझे कीड़ों को छूने से कोई परेशानी नहीं होती थी पर अब मैं उनकी तस्वीरें देखना भी बर्दाश्त नहीं कर सकता ।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlW2z7o0eEWw",
        "colab_type": "text"
      },
      "source": [
        "We will add an start and end token to the Target Sentences so that the model know the start and the end of the sentence. It also help while decoding using an trained model where we stop sampling when a stop character occur.\n",
        "Here we will be adding **START_** as the start of the sentence token and **END_** for indicating end of the sentence in the target data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naJLThN7bTvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "00c1c212-e169-4554-c08d-a74c36a0467d"
      },
      "source": [
        "#adding START_ and END_ tokens\n",
        "print('Before adding tokens:',df.iloc[277,1])\n",
        "df['Hindi']=df['Hindi'].apply(lambda x: 'start_ '+ x + ' end_')\n",
        "print('After adding tokens:',df.iloc[277,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before adding tokens: यह सही नहीं है ।\n",
            "After adding tokens: start_ यह सही नहीं है । end_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtQFV5R-p9-R",
        "colab_type": "text"
      },
      "source": [
        "We can't feed text data directly into the Neural Network. For feeding a text into NN we will have to **Tokenize** it and further preprocess the data.\n",
        "What we basically doing is we will be creating a python dictionary of all the word in the Hindi and English text data and based on the position in the dictionary a word will be mapped to a integer(called index). Every sentence will be transformed into a list of integer representing each words in that sentence.\n",
        "for example \"I am a very good boy\" will be translated to [345 543 56 956 2000 400].\n",
        "ps:integer values are only for understanding not per real vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf5D2vldp8NB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "fac6cfdc-36fa-462b-f979-cc0913ac559a"
      },
      "source": [
        "#we will need the average or the maximum length of the input and target sequences \n",
        "#and the number of words in the two vocab\n",
        "df['len_Hindi_split']=df['Hindi'].apply(lambda x:len(x.split(' ')))\n",
        "df['len_English_split']=df['English'].apply(lambda x:len(x.split(' ')))\n",
        "max_length_src=max((df['len_English_split']))\n",
        "max_length_tar=max((df['len_Hindi_split']))\n",
        "print('Maximul length of source(max_len_src):',max_length_src)\n",
        "print('Maximul length of target(max_len_tar):',max_length_tar)\n",
        "\n",
        "# create word set for hindi and english vocabulary\n",
        "eng_vocab=set()\n",
        "for line in df['English']:\n",
        "  for w in line.split():\n",
        "    if w not in eng_vocab:\n",
        "      eng_vocab.add(w)\n",
        "hin_vocab=set()\n",
        "for line in df['Hindi']:\n",
        "  for w in line.split():\n",
        "    if w not in hin_vocab:\n",
        "      hin_vocab.add(w)\n",
        "\n",
        "#calculate the length of this two vocab.\n",
        "len_src_vocab=len(eng_vocab)\n",
        "len_tar_vocab=len(hin_vocab)\n",
        "print('length of source vocab(len_src_vocab):',len(eng_vocab))\n",
        "print('length of target vocab(len_tar_vocab):',len(hin_vocab))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximul length of source(max_len_src): 25\n",
            "Maximul length of target(max_len_tar): 28\n",
            "length of source vocab(len_src_vocab): 2319\n",
            "length of target vocab(len_tar_vocab): 2838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUh-S6F31zfi",
        "colab_type": "text"
      },
      "source": [
        "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
        "For further information on tokenizer please visit the following\n",
        "[link](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PvOsS_N0qWD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "875e9a41-6a2c-4b5b-c307-b7030043f577"
      },
      "source": [
        "# Instantiate an tokeziner for INPUT sequence\n",
        "eng_tokenizer=Tokenizer(filters='',oov_token='<unk>')\n",
        "eng_tokenizer.fit_on_texts(df['English'])\n",
        "# Get our input sequence  word index\n",
        "eng_word_index = eng_tokenizer.word_index\n",
        "# create input tensor\n",
        "input_tensor = eng_tokenizer.texts_to_sequences(df['English'])\n",
        "input_tensor = pad_sequences(input_tensor,padding='post')\n",
        "\n",
        "\n",
        "# Instantiate an tokeziner for TARGET sequence\n",
        "hin_tokenizer=Tokenizer(filters='',oov_token='<unk>')\n",
        "hin_tokenizer.fit_on_texts(df['Hindi'])\n",
        "# Get our input sequence  word index\n",
        "hin_word_index = hin_tokenizer.word_index\n",
        "# create input tensor\n",
        "target_tensor = hin_tokenizer.texts_to_sequences(df['Hindi'])\n",
        "target_tensor = pad_sequences(target_tensor,padding='post')\n",
        "\n",
        "print('input_tensor[0]:',input_tensor[0])\n",
        "print('target_tensor[0]:',target_tensor[0])\n",
        "\n",
        "#lets break the input and target tensor into the training and tvalidation data\n",
        "input_tensor_train,input_tensor_val,target_tensor_train,target_tensor_val=train_test_split(input_tensor,\n",
        "                                                                                            target_tensor,test_size=0.15)\n",
        "print('\\n\\n','input_tensor_train:',len(input_tensor_train), '  target_tensor_train:',len(target_tensor_train), \n",
        "      '\\n input_tensor_val:',len(input_tensor_val),'     target_tensor_val:' ,len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_tensor[0]: [1244   59    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0]\n",
            "target_tensor[0]: [   2 1407   73    3    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "\n",
            "\n",
            " input_tensor_train: 2361   target_tensor_train: 2361 \n",
            " input_tensor_val: 417      target_tensor_val: 417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYbjBTuFUTVf",
        "colab_type": "text"
      },
      "source": [
        "Now usig **tf.data.Dataset** we will build a input pipeline to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfpPB6kqVG1p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c4dd6a2-881f-4043-fb5e-6bcef34712cf"
      },
      "source": [
        "buffer_size=len(input_tensor_train)\n",
        "#embedding dimension is a legth of vector in which each word of the sequence will be transformed\n",
        "embedding_dim=256\n",
        "Batch_size=64\n",
        "steps_per_epoch = len(input_tensor_train)//Batch_size\n",
        "units=40\n",
        "\n",
        "vocab_inp_size = len_src_vocab+1\n",
        "vocab_tar_size = len_tar_vocab+1\n",
        "\n",
        "dataset=tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(buffer_size)\n",
        "dataset=dataset.batch(Batch_size, drop_remainder=True)\n",
        "\n",
        "#lets print one iteration of the batch \n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 25]), TensorShape([64, 28]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I94G7SrjldQF",
        "colab_type": "text"
      },
      "source": [
        "Implementing Artchitecture for **Encoder Decoder with attention** based mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXKA-wMxevzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz=batch_sz\n",
        "    self.enc_units=enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True,recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh1zqxYzjyhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "feed27a0-744f-4d57-c5c0-67e246de6bee"
      },
      "source": [
        "encoder=Encoder(vocab_inp_size+1, embedding_dim, units, Batch_size)\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder state shape: (batch size, units) {}'.format(sample_hidden.shape))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 25, 40)\n",
            "Encoder state shape: (batch size, units) (64, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQnKprHI6nDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RgqtwdJ5eo-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "88899c59-9762-42be-f46b-86ac3ddd06a5"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 40)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 25, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoe7EC_D5mGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVrYddx3jF1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ce068239-5f90-475f-ada5-eb456d01caa7"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size+1, embedding_dim, units, Batch_size)\n",
        "\n",
        "sample_decoder_output, state, attention_weights = decoder(tf.random.uniform((Batch_size, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
        "print ('attention_weights.shape: ',attention_weights.shape)\n",
        "print ('state.shape:',state.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 2840)\n",
            "attention_weights.shape:  (64, 25, 1)\n",
            "state.shape: (64, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwbM6PF6joZs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8ca3627-5df2-4670-8d05-534e8b5f68a4"
      },
      "source": [
        "attention_weights.shape,state.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 25, 1]), TensorShape([64, 40]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKUOw8KP8xYJ",
        "colab_type": "text"
      },
      "source": [
        "##Optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtqgsfPr8PDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE23PmbE-iw-",
        "colab_type": "text"
      },
      "source": [
        "Create a Check Point "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2AI8OFo8PPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YtF3rlc-xq3",
        "colab_type": "text"
      },
      "source": [
        "##Training steps\n",
        "\n",
        "Pass the input through the encoder which return encoder output and the encoder \n",
        "\n",
        "1.   Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
        "2.   The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
        "3.   The decoder returns the predictions and the decoder hidden state.\n",
        "4.   The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5.   Use teacher forcing to decide the next input to the decoder.\n",
        "Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
        "6.   The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRl99Dss8PBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a train_step function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([hin_tokenizer.word_index['start_']] * Batch_size, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rob3zzcT5mWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "#start the training \n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWX7wb9IizRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk4W_kdGxlTx",
        "colab_type": "text"
      },
      "source": [
        "#                   **INFERENCE**\n",
        "Inferencing is slightly different from the training process for NMT (Figure 10.11). As we do not have a target sentence at the inference time, we need a way to trigger the decoder at the end of the encoding phase. This shares similarities with the image captioning exercise we did in Chapter 9, Applications of LSTM – Image Caption Generation. In that exercise, we appended the <SOS> token to the beginning of the captions to denote the start of the caption and <EOS> to denote the end.\n",
        "\n",
        "We can simply do this by giving **\"start_\"** as the first input to the decoder, then by getting the prediction as the output, and by feeding in the last prediction as the next input to the NMT, we stop the prediction/sampling when we get **\"end_\"** as the next sample.\n",
        "\n",
        "\n",
        "<img src=\"https://static.packt-cdn.com/products/9781788478311/graphics/B08681_10_73.jpg\" width=\"1000\" alt=\"attention mechanism\">\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ4K5c1BizOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here in the evaluate function we also need to process the Raw input sentence almost same preprocessing we did to the input dataset\n",
        "def evaluate(sentence):\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da87Lm3c9E8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence=\"Ram is 2 [ good boy.\"\n",
        "sentence=re.sub(r\"([.!?¿])\",r\" \\1\",sentence)\n",
        "sentence=re.sub(r\"([^a-zA-Z.!?¿]+)\",' ' ,sentence)\n",
        "sentence=re.sub(r'([\" \"]+)',' ',sentence)\n",
        "sentence=sentence.lower()\n",
        "sentence=sentence.strip()\n",
        "a= []\n",
        "for w in sentence.split():\n",
        "  if  eng_word_index.get(w):\n",
        "    a.append(eng_word_index[w])\n",
        "  else:\n",
        "    a.append(eng_word_index['<unk>'])\n",
        "sentence_token = tf.keras.preprocessing.sequence.pad_sequences([a],maxlen=max_length_src,padding='post')\n",
        "sentence_tensor=inputs = tf.convert_to_tensor(sentence_token)\n",
        "\n",
        "hidden = [tf.zeros((1, units))]\n",
        "enc_out, enc_hidden = encoder(inputs, hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx8kgf-VGXNi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61cf4d0e-3160-40b0-dae4-6bdb7ce0c9f7"
      },
      "source": [
        "enc_hidden.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 40])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3bojYIfGX2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckFbiF8nGYBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCKspWlmGYLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_KksfvrewYQ",
        "colab_type": "text"
      },
      "source": [
        "For further information:\n",
        "\n",
        "*   [Neural machine translation with attention](https://www.tensorflow.org/)\n",
        "*   [Neural Machine Translation (seq2seq) Tutorial](https://github.com/tensorflow/nmt)\n"
      ]
    }
  ]
}